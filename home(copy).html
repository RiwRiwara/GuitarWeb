<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
    <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hand_gesture_recognizer"></script>
    <title>Hand Gesture Recognizer</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="../static/js/script.js"></script>
    
</head>
    
<body class="bg-gray-100">
    
    <header class="bg-white">
        <div class="flex justify-between items-center shadow-md px-10 py-6">
            <div class="flex items-center">
                <div class="w-5 h-5 bg-black rounded-full mr-2"></div>
                <h1 class="text-2xl font-bold text-primary-950">Guitar Music</h1>
            </div>
            <div class="flex space-x-4">
                <a href="home.html" class="nav-link text-black font-semibold">Home</a>
                <a href="song.html" class="nav-link text-gray-500 font-semibold">Song Library</a>
            </div>
        </div>
    </header>
    
    <section class="flex flex-col gap-2 my-4 p-2">
        <div class="text-3xl text-black font-bold text-center mt-14 mb-2">Welcome to Guitar Music</div>
        <div class="text-base text-gray-400 font-normal text-center">Let's try your first move with our model.</div>
    </section>

    <section id="demos">
        <div id="liveView" class="videoView">
            <button id="webcamButton" class="flex items-center">
                <!-- <span class="mdc-button__ripple"></span>
                <span class="mdc-button__label">OPEN WEBCAM</span> -->

                <div class="flex justify-center">
                    <div class="relative inline-block w-14 h-8">
                        <input type="checkbox" id="toggle-button" class="hidden">
                        <label for="toggle-button" class="cursor-pointer">
                            <span class="block bg-gray-300 w-14 h-8 rounded-full"></span>
                            <span class="absolute left-1 top-1 bg-white w-6 h-6 rounded-full transition-transform duration-300 transform translate-x-0"></span>
                        </label>
                    </div>
                </div>

                <div class="text-lg text-black font-semibold text-center ml-4">Enable predictions</div>
            </button>
            
            <div style="position: relative;">
                <video id="webcam" autoplay playsinline></video>
                <canvas class="output_canvas" id="output_canvas" width="1280" height="720"
                    style="position: absolute; left: 0px; top: 0px;"></canvas>
                <p id="gesture_output" class="output"></p>
            </div>
        </div>
    </section>

    <script type="module">
        import {
            GestureRecognizer,
            FilesetResolver,
            DrawingUtils
        } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

        const demosSection = document.getElementById("demos");
        let gestureRecognizer;
        let runningMode = "IMAGE";
        let enableWebcamButton;
        let webcamRunning = false;
        const videoHeight = "360px";
        const videoWidth = "480px";

        // get everything needed to run.
        const createGestureRecognizer = async () => {
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
            );
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath:
                        //"https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                        "{{ url_for('static', filename='gesture_recognizer.task') }}",
                    delegate: "GPU"
                },
                runningMode: runningMode
            });
            demosSection.classList.remove("invisible");
        };
        createGestureRecognizer();


        /********************************************************************
        // Demo 2: Continuously grab image from webcam stream and detect it.
        ********************************************************************/

        const video = document.getElementById("webcam");
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const gestureOutput = document.getElementById("gesture_output");

        // Check if webcam access is supported.
        function hasGetUserMedia() {
            return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
        }

        // If webcam supported, add event listener to button for when user
        // wants to activate it.
        if (hasGetUserMedia()) {
            enableWebcamButton = document.getElementById("webcamButton");
            enableWebcamButton.addEventListener("click", enableCam);
        } else {
            console.warn("getUserMedia() is not supported by your browser");
        }

        // Enable the live webcam view and start detection.
        function enableCam(event) {
            if (!gestureRecognizer) {
                alert("Please wait for gestureRecognizer to load");
                return;
            }

            if (webcamRunning === true) {
                webcamRunning = false;
                enableWebcamButton.innerText = "Enable Predictions";
            } else {
                webcamRunning = true;
                enableWebcamButton.innerText = "Disable Predictions";
            }

            // getUsermedia parameters.
            const constraints = {
                video: true
            };

            // Activate the webcam stream.
            navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
                video.srcObject = stream;
                video.addEventListener("loadeddata", predictWebcam);
            });
        }

        let lastVideoTime = -1;
        let results = undefined;
        async function predictWebcam() {
            const webcamElement = document.getElementById("webcam");
            // Now let's start detecting the stream.
            if (runningMode === "IMAGE") {
                runningMode = "VIDEO";
                await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
            }
            let nowInMs = Date.now();
            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                results = gestureRecognizer.recognizeForVideo(video, nowInMs);
            }

            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            const drawingUtils = new DrawingUtils(canvasCtx);

            canvasElement.style.height = videoHeight;
            webcamElement.style.height = videoHeight;
            canvasElement.style.width = videoWidth;
            webcamElement.style.width = videoWidth;

            if (results.landmarks) {
                for (const landmarks of results.landmarks) {
                    drawingUtils.drawConnectors(
                        landmarks,
                        GestureRecognizer.HAND_CONNECTIONS,
                        {
                            color: "#00FF00",
                            lineWidth: 5
                        }
                    );
                    drawingUtils.drawLandmarks(landmarks, {
                        color: "#FF0000",
                        lineWidth: 2
                    });
                }
            }
            canvasCtx.restore();
            if (results.gestures.length > 0) {
                gestureOutput.style.display = "block";
                gestureOutput.style.width = videoWidth;
                const categoryName = results.gestures[0][0].categoryName;
                const categoryScore = parseFloat(
                    results.gestures[0][0].score * 100
                ).toFixed(2);
                const handedness = results.handednesses[0][0].displayName;
                gestureOutput.innerText = `GestureRecognizer: ${categoryName}\n Confidence: ${categoryScore} %\n Handedness: ${handedness}`;
            } else {
                gestureOutput.style.display = "none";
            }
            // Call this function again to keep predicting when the browser is ready.
            if (webcamRunning === true) {
                window.requestAnimationFrame(predictWebcam);
            }
        }
    </script>

</body>

</html>